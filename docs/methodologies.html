<!DOCTYPE html>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<title>Methodologies | EMS24</title>
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:ital,wght@0,400;0,600;0,700;1,400;1,600;1,700&amp;display=swap" crossorigin>
<link rel="preload" as="style" href="./_observablehq/theme-air,near-midnight.css">
<link rel="preload" as="style" href="./_npm/katex@0.16.11/dist/katex.min.css">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:ital,wght@0,400;0,600;0,700;1,400;1,600;1,700&amp;display=swap" crossorigin>
<link rel="stylesheet" type="text/css" href="./_observablehq/theme-air,near-midnight.css">
<link rel="stylesheet" type="text/css" href="./_npm/katex@0.16.11/dist/katex.min.css">
<link rel="modulepreload" href="./_observablehq/client.js">
<link rel="modulepreload" href="./_observablehq/runtime.js">
<link rel="modulepreload" href="./_observablehq/stdlib.js">
<link rel="modulepreload" href="./_npm/@observablehq/plot@0.6.15/_esm.js">
<link rel="modulepreload" href="./_observablehq/stdlib/tex.js">
<link rel="modulepreload" href="./_npm/katex@0.16.11/_esm.js">
<link rel="modulepreload" href="./_npm/d3@7.9.0/_esm.js">
<link rel="modulepreload" href="./_npm/isoformat@0.2.1/_esm.js">
<link rel="modulepreload" href="./_npm/interval-tree-1d@1.0.4/_esm.js">
<link rel="modulepreload" href="./_npm/d3-array@3.2.4/_esm.js">
<link rel="modulepreload" href="./_npm/d3-axis@3.0.0/_esm.js">
<link rel="modulepreload" href="./_npm/d3-brush@3.0.0/_esm.js">
<link rel="modulepreload" href="./_npm/d3-chord@3.0.1/_esm.js">
<link rel="modulepreload" href="./_npm/d3-color@3.1.0/_esm.js">
<link rel="modulepreload" href="./_npm/d3-contour@4.0.2/_esm.js">
<link rel="modulepreload" href="./_npm/d3-delaunay@6.0.4/_esm.js">
<link rel="modulepreload" href="./_npm/d3-dispatch@3.0.1/_esm.js">
<link rel="modulepreload" href="./_npm/d3-drag@3.0.0/_esm.js">
<link rel="modulepreload" href="./_npm/d3-dsv@3.0.1/_esm.js">
<link rel="modulepreload" href="./_npm/d3-ease@3.0.1/_esm.js">
<link rel="modulepreload" href="./_npm/d3-fetch@3.0.1/_esm.js">
<link rel="modulepreload" href="./_npm/d3-force@3.0.0/_esm.js">
<link rel="modulepreload" href="./_npm/d3-format@3.1.0/_esm.js">
<link rel="modulepreload" href="./_npm/d3-geo@3.1.1/_esm.js">
<link rel="modulepreload" href="./_npm/d3-hierarchy@3.1.2/_esm.js">
<link rel="modulepreload" href="./_npm/d3-interpolate@3.0.1/_esm.js">
<link rel="modulepreload" href="./_npm/d3-path@3.1.0/_esm.js">
<link rel="modulepreload" href="./_npm/d3-polygon@3.0.1/_esm.js">
<link rel="modulepreload" href="./_npm/d3-quadtree@3.0.1/_esm.js">
<link rel="modulepreload" href="./_npm/d3-random@3.0.1/_esm.js">
<link rel="modulepreload" href="./_npm/d3-scale@4.0.2/_esm.js">
<link rel="modulepreload" href="./_npm/d3-scale-chromatic@3.1.0/_esm.js">
<link rel="modulepreload" href="./_npm/d3-selection@3.0.0/_esm.js">
<link rel="modulepreload" href="./_npm/d3-shape@3.2.0/_esm.js">
<link rel="modulepreload" href="./_npm/d3-time@3.1.0/_esm.js">
<link rel="modulepreload" href="./_npm/d3-time-format@4.1.0/_esm.js">
<link rel="modulepreload" href="./_npm/d3-timer@3.0.1/_esm.js">
<link rel="modulepreload" href="./_npm/d3-transition@3.0.1/_esm.js">
<link rel="modulepreload" href="./_npm/d3-zoom@3.0.0/_esm.js">
<link rel="modulepreload" href="./_npm/binary-search-bounds@2.0.5/_esm.js">
<link rel="modulepreload" href="./_npm/internmap@2.0.3/_esm.js">
<link rel="modulepreload" href="./_npm/delaunator@5.0.1/_esm.js">
<link rel="modulepreload" href="./_npm/robust-predicates@3.0.2/_esm.js">
<link rel="icon" href="./_file/observable.1af93621.png" type="image/png" sizes="32x32">
<script type="module">

import {define} from "./_observablehq/client.js";
import {registerFile} from "./_observablehq/stdlib.js";

registerFile("./data/drn_rasp_lerch.png", {"name":"./data/drn_rasp_lerch.png","mimeType":"image/png","path":"./_file/data/drn_rasp_lerch.674541c2.png","lastModified":1721109345988});
registerFile("./data/improver_training.json", {"name":"./data/improver_training.json","mimeType":"application/json","path":"./_file/data/improver_training.67601bdf.json","lastModified":1721125351114});

define({id: "598b833e", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`\mu`
))
}});

define({id: "33a3efc4", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`\sigma`
))
}});

define({id: "1c5eabda", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex.block`
   \mathcal{N}(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right),
  `
))
}});

define({id: "598b833e-1", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`\mu`
))
}});

define({id: "691ff5e5", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`\sigma^2`
))
}});

define({id: "33a3efc4-1", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`\sigma`
))
}});

define({id: "4b93f57a", inputs: ["FileAttachment"], outputs: ["pme_models"], body: (FileAttachment) => {
const pme_models = FileAttachment("./data/improver_training.json").json()
return {pme_models};
}});

define({id: "4d51eb57", inputs: ["Plot","pme_models","display"], body: async (Plot,pme_models,display) => {
display(await(
Plot.plot({
  x: {
    axis: "bottom",
    grid: true,
    label: "Days"
  },
  y: {
    grid: true,
    label: ""
  },
  marks: [
    Plot.barX(pme_models, {
      x: "value",
      y: "category",
      fill: "type",
      title: d => `${d.type}: ${d.value}`
    })
  ],
  color: {legend: true, domain: ["Training", "Forecast"]}
})
))
}});

define({id: "2d35f10f", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex.block`
    \mathcal{N}(a + b_1X_1 + ... + b_mX_m, \sqrt{c + dS^{2}}),
    `
))
}});

define({id: "598b833e-2", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`\mu`
))
}});

define({id: "8283c9d2", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`b_i`
))
}});

define({id: "8a6928ae", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`a`
))
}});

define({id: "8283c9d2-1", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`b_i`
))
}});

define({id: "107f4b02", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`c`
))
}});

define({id: "13d535b5", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`d`
))
}});

define({id: "598b833e-3", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`\mu`
))
}});

define({id: "33a3efc4-2", mode: "inline", inputs: ["tex","display"], body: async (tex,display) => {
display(await(
tex`\sigma`
))
}});

define({id: "6cab1a6e", inputs: ["FileAttachment","display"], body: async (FileAttachment,display) => {
display(await(
FileAttachment("./data/drn_rasp_lerch.png").image({width: 550, style: "border: 6px solid white; border-radius: 1%;"})
))
}});

</script>
<input id="observablehq-sidebar-toggle" type="checkbox" title="Toggle sidebar">
<label id="observablehq-sidebar-backdrop" for="observablehq-sidebar-toggle"></label>
<nav id="observablehq-sidebar">
  <ol>
    <label id="observablehq-sidebar-close" for="observablehq-sidebar-toggle"></label>
    <li class="observablehq-link"><a href="./">EMS24</a></li>
  </ol>
  <section class="observablehq-section-active">
    <summary>Contents</summary>
    <ol>
    <li class="observablehq-link"><a href="./introduction">Introduction</a></li>
    <li class="observablehq-link"><a href="./study_zone">Study site</a></li>
    <li class="observablehq-link"><a href="./data">Data</a></li>
    <li class="observablehq-link observablehq-link-active"><a href="./methodologies">Methodologies</a></li>
    <li class="observablehq-link"><a href="./results">Results</a></li>
    <li class="observablehq-link"><a href="./results_dashboard">Results Dashboard</a></li>
    <li class="observablehq-link"><a href="./conclusions">Conclusions</a></li>
    </ol>
  </section>
</nav>
<script>{Object.assign(document.createElement("a"),{href:""}).password&&location.replace(location.href);const e=document.querySelector("#observablehq-sidebar"),t=document.querySelector("#observablehq-sidebar-toggle"),r=sessionStorage.getItem("observablehq-sidebar");r?t.checked=r==="true":t.indeterminate=!0;for(const o of document.querySelectorAll("#observablehq-sidebar summary")){const s=o.parentElement;switch(sessionStorage.getItem(`observablehq-sidebar:${o.textContent}`)){case"true":s.open=!0;break;case"false":s.classList.contains("observablehq-section-active")||(s.open=!1);break}}addEventListener("beforeunload",()=>sessionStorage.setItem("observablehq-sidebar-scrolly",`${e.scrollTop}`));const a=sessionStorage.getItem("observablehq-sidebar-scrolly");a!=null&&(e.style.cssText="overflow: hidden;",e.scrollTop=+a,e.style.cssText="");}</script>
<aside id="observablehq-toc" data-selector="h1:not(:first-of-type)[id], h2:first-child[id], :not(h1) + h2[id]">
<nav>
<div>Contents</div>
<ol>
<li class="observablehq-secondary-link"><a href="#ensemble-model-output-statistics">Ensemble Model Output Statistics</a></li>
<li class="observablehq-secondary-link"><a href="#distributional-regression-neural-networks">Distributional Regression Neural Networks</a></li>
</ol>
</nav>
</aside>
<div id="observablehq-center">
<main id="observablehq-main" class="observablehq">
<h1 id="methodologies" tabindex="-1"><a class="observablehq-header-anchor" href="#methodologies">Methodologies</a></h1>
<p>Post-processing ensemble forecasts is usually approached using two main techniques: Bayesian Model Averaging
(BMA; <a href="#references">Raftery et al., 2005</a>) and non-homogeneous regression (EMOS; <a href="#references">Gneiting et al., 2005</a>), both of which
calibrate parametric forecast distributions. Therefore, a distribution has to be chosen in order to adjust
and calibrate its parameters.</p>
<p>Since this project involves working with temperature forecasts, a Gaussian or
normal distribution can be assumed, with location (<observablehq-loading></observablehq-loading><!--:598b833e:-->) and scale (<observablehq-loading></observablehq-loading><!--:33a3efc4:-->) parameters derived from the mean and standard
deviation of the ensemble or from the individual ensemble members.</p>
<p><observablehq-loading></observablehq-loading><!--:1c5eabda:--></p>
<p>where:</p>
<ul>
<li><observablehq-loading></observablehq-loading><!--:598b833e-1:--> is the mean</li>
<li><observablehq-loading></observablehq-loading><!--:691ff5e5:--> is the variance</li>
<li><observablehq-loading></observablehq-loading><!--:33a3efc4-1:--> is the standard deviation</li>
</ul>
<p>For this work, two different strategies were tested to post-process temperature ensemble forecasts. One is
EMOS using the implementation in IMPROVER, and the other is Distributional Regression Neural Networks proposed
by <a href="#references">Rasp and Lerch (2018)</a>.</p>
<h2 id="ensemble-model-output-statistics" tabindex="-1"><a class="observablehq-header-anchor" href="#ensemble-model-output-statistics">Ensemble Model Output Statistics</a></h2>
<p>IMPROVER, developed by the Met Office, includes a module that implements EMOS with different distributions, allowing
the use of either the mean and standard deviation of an ensemble or the individual realizations of the ensemble
(<a href="#references">Roberts et al., 2023</a>). In this study, only the latter approach was tested, as we have a multi-model
ensemble and, a priori, the members are not equally probable.</p>
<p>This methodology is often applied using recent data, which is also the case in this study. A rolling training period
of 45 days is used, meaning that a forecast for today is obtained with a model trained on data from the past 45 days.</p>
<div class="observablehq observablehq--block"><!--:4b93f57a:--></div>
<div class="observablehq observablehq--block"><observablehq-loading></observablehq-loading><!--:4d51eb57:--></div>
<p>As mentioned in the <a href="./data">Data</a> section, there are not the same number of models for all
lead times, as three of them end at 48 hours of lead time. However, since the strategy involves
calculating a calibration correction for each lead time and station, this was not a problem.
From 0 to 48 lead times, 10 models are used to calculate the EMOS coefficients, and from 49 to
72, only 7 models are used. This methodology is referred to in this project as <em>EMOS-IMPROVER</em>.</p>
<p>Since the values of the ensemble members are used instead of the mean and standard deviation,
the normal distribution is defined as follows:</p>
<p><observablehq-loading></observablehq-loading><!--:2d35f10f:--></p>
<p>where the location parameter <observablehq-loading></observablehq-loading><!--:598b833e-2:--> is the weighted mean of the ensemble forecasts
(with weights <observablehq-loading></observablehq-loading><!--:8283c9d2:-->). Parameters <observablehq-loading></observablehq-loading><!--:8a6928ae:-->, <observablehq-loading></observablehq-loading><!--:8283c9d2-1:-->, <observablehq-loading></observablehq-loading><!--:107f4b02:--> and <observablehq-loading></observablehq-loading><!--:13d535b5:--> are determined
using the training data and by minimising the Continuous Ranked Probability Score (CRPS).</p>
<p>To obtain a calibrated forecast, the parameters can be used to determine the location and scale parameter
of the normal distribution. More information can be found at
<a href="https://improver.readthedocs.io/en/latest/improver.calibration.ensemble_calibration.html#ensemble-model-output-statistics-emos" target="_blank" rel="noopener noreferrer">EMOS - IMPROVER</a>.</p>
<hr>
<h2 id="distributional-regression-neural-networks" tabindex="-1"><a class="observablehq-header-anchor" href="#distributional-regression-neural-networks">Distributional Regression Neural Networks</a></h2>
<p>The estimation of the calibrated distribution parameters <observablehq-loading></observablehq-loading><!--:598b833e-3:--> and <observablehq-loading></observablehq-loading><!--:33a3efc4-2:--> can also be
performed using neural networks. <a href="#references">Rasp and Lerch (2018)</a> proposed using neural networks
for distributional regression tasks. In their study, the ECMWF ensemble was postprocessed with a
fully connected neural network, achieving a 30% CRPS reduction with DRN compared to the raw ECMWF ensemble.</p>
<p>The network used the ensemble mean and standard deviation as input features, together with station
embeddings. The latter allowed the network to learn station-specific information (<a href="#references">Rasp and Lerch (2018)</a>).
Additionally, some auxiliary input features were used, such as convective available potential energy,
cloud cover, soil moisture, altitude of the station, and station location coordinates.</p>
<div class="observablehq observablehq--block"><observablehq-loading></observablehq-loading><!--:6cab1a6e:--></div>
<p>In this study, the idea of using neural networks for distributional regression tasks is also employed,
following a similar approach to that of <a href="#references">Rasp and Lerch (2018)</a>. Since the Poor Man's
Ensemble (<a href="./data#numerical-weather-prediction-model">PME</a>) is used, which involves different models,
some changes were implemented:</p>
<ul>
<li>Use of station embeddings with more than two dimensions.</li>
<li>No auxiliary variables, apart from 2-m temperature forecasts, are used as input features.</li>
<li>More hidden layers are considered for the architecture of the neural network.</li>
<li>Ensemble members are also considered as input features, rather than just their mean and standard deviation.</li>
<li>Use of a dropout layer.</li>
</ul>
<p>The optimisation of embeddings dimensions, number of hidden layers, number of units per layer, and dropout rate was
done using <a href="https://optuna.org/" target="_blank" rel="noopener noreferrer">Optuna</a>. The DRN techniques were implemented using Python libraries
Tensorflow (<a href="#references">Abadi et al. 2016</a>) and Keras (<a href="#references">Chollet et al. 2021</a>). The loss function used
is the same as in <a href="#references">Rasp and Lerch (2018)</a>, CRPS.</p>
<p>The training of the DRN was done in the Kaggle environment. The Adam optimizer was used, together with an early
stopping configuration to avoid overfitting and a reduction in the learning rate when the loss reached a plateau.</p>
<h3 id="drn-approaches" tabindex="-1"><a class="observablehq-header-anchor" href="#drn-approaches">DRN approaches</a></h3>
<p>As aforementioned, the use of the Poor Man's Ensemble (PME) involves different models and is not an ensemble built
from perturbations of a single model. Therefore, a DRN approach may benefit from using ensemble members rather than
just their mean and standard deviation. Additionally, as presented in the Data section, not all lead times have the
same number of models, which poses a challenge for defining a single DRN for the entire postprocessing of PME.
Therefore, two approaches were proposed:</p>
<ul>
<li>
<p><em>DRN-Mean</em>: The mean and standard deviation of the ensemble are used as input features. This approach has the
advantage of being independent of how the mean and standard deviation are obtained, so a single DRN can be trained.</p>
</li>
<li>
<p><em>DRN-Members</em>: The ensemble members are used as input features. This approach requires the training of two DRNs,
one for the 0-48 lead times and another for the 49-72 lead times, since a different number of models are available
for each set of lead times.</p>
</li>
</ul>
<hr>
<h4 id="references" tabindex="-1"><a class="observablehq-header-anchor" href="#references">References</a></h4>
<span style="font-size:0.85em;">
<p>Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., ... &amp; Zheng, X. (2016). TensorFlow: a system for Large-Scale machine learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16) (pp. 265-283).</p>
<p>Chollet, F. (2021). Deep learning with Python. Simon and Schuster.</p>
<p>Gneiting, T., Raftery, A. E., Westveld, A. H., &amp; Goldman, T. (2005). Calibrated probabilistic forecasting using ensemble model output statistics and minimum CRPS estimation. Monthly Weather Review, 133(5), 1098-1118.</p>
<p>Raftery, A. E., Gneiting, T., Balabdaoui, F., &amp; Polakowski, M. (2005). Using Bayesian model averaging to calibrate forecast ensembles. Monthly weather review, 133(5), 1155-1174.</p>
<p>Rasp, S., &amp; Lerch, S. (2018). Neural networks for postprocessing ensemble weather forecasts. Monthly Weather Review, 146(11), 3885-3900.</p>
<p>Roberts, N., Ayliffe, B., Evans, G., Moseley, S., Rust, F., Sandford, C., ... &amp; Worsfold, M. (2023). IMPROVER: the new probabilistic postprocessing system at the Met Office. Bulletin of the American Meteorological Society, 104(3), E680-E697.</p>
</span></main>
<footer id="observablehq-footer">
<nav><a rel="prev" href="./data"><span>Data</span></a><a rel="next" href="./results"><span>Results</span></a></nav>
<div>Built with <a href="https://observablehq.com/" target="_blank" rel="noopener noreferrer">Observable</a> on <a title="2024-07-22T11:36:34">Jul 22, 2024</a>.</div>
</footer>
</div>
